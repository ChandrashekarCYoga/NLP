{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "NLP Basics-Implementing a pipeline to clean text.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandrashekarCYoga/NLP/blob/master/NLP_Basics_Implementing_a_pipeline_to_clean_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNtsw4jnZEdC",
        "colab_type": "text"
      },
      "source": [
        "# NLP Basics: Implementing a pipeline to clean text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlsHNUc0HAh4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOqfYGPVZEdF",
        "colab_type": "text"
      },
      "source": [
        "### Pre-processing text data\n",
        "\n",
        "Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n",
        "1. **Remove punctuation**\n",
        "2. **Tokenization**\n",
        "3. **Remove stopwords**\n",
        "4. **Lemmatize/Stem**\n",
        "\n",
        "The first three steps are covered in this chapter as they're implemented in pretty much any text cleaning pipeline. Lemmatizing and stemming are covered in the next chapter as they're helpful but not critical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiasfVykZEdI",
        "colab_type": "code",
        "outputId": "8db59c13-97b2-4034-983d-a2fbc15c82f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
        "data.columns = ['label', 'body_text']\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                            body_text\n",
              "0   ham  I've been searching for the right words to thank you for this breather. I promise i wont take yo...\n",
              "1  spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
              "2   ham                                        Nah I don't think he goes to usf, he lives around here though\n",
              "3   ham                        Even my brother is not like to speak with me. They treat me like aids patent.\n",
              "4   ham                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sxc0lh3ZEdQ",
        "colab_type": "code",
        "outputId": "bc828482-f3ae-462f-8535-e25f5601318f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# What does the cleaned version look like?\n",
        "data_cleaned = pd.read_csv(\"SMSSpamCollection_cleaned.tsv\", sep='\\t')\n",
        "data_cleaned.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_nostop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>['ive', 'searching', 'right', 'words', 'thank', 'breather', 'promise', 'wont', 'take', 'help', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>['date', 'sunday']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                     body_text_nostop\n",
              "0   ham  ...  ['ive', 'searching', 'right', 'words', 'thank', 'breather', 'promise', 'wont', 'take', 'help', '...\n",
              "1  spam  ...  ['free', 'entry', '2', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005...\n",
              "2   ham  ...                                 ['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']\n",
              "3   ham  ...                              ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent']\n",
              "4   ham  ...                                                                                   ['date', 'sunday']\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYd4rF5MZEdZ",
        "colab_type": "text"
      },
      "source": [
        "### Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS1-VlR5ZEdz",
        "colab_type": "code",
        "outputId": "7195215b-8455-4cea-d62e-d204fbd27254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPma3-k2ZEeA",
        "colab_type": "code",
        "outputId": "6e2ac276-aa6c-4e94-d1b8-d0d821820350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"I like NLP.\" == \"I like NLP\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9UTr-eZEeD",
        "colab_type": "code",
        "outputId": "e1c04f45-ab3a-43b9-fc39-f5a724026326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def remove_punct(text):\n",
        "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return text_nopunct\n",
        "\n",
        "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                      body_text_clean\n",
              "0   ham  ...  Ive been searching for the right words to thank you for this breather I promise i wont take your...\n",
              "1  spam  ...  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...\n",
              "2   ham  ...                                          Nah I dont think he goes to usf he lives around here though\n",
              "3   ham  ...                          Even my brother is not like to speak with me They treat me like aids patent\n",
              "4   ham  ...                                                                    I HAVE A DATE ON SUNDAY WITH WILL\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkBouCTQZEeH",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMKeaAJmZEeH",
        "colab_type": "code",
        "outputId": "a78ebeb3-b524-489d-861a-0013e1ec858c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
              "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
              "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
              "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                  body_text_tokenized\n",
              "0   ham  ...  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...\n",
              "1  spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...\n",
              "2   ham  ...                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]\n",
              "3   ham  ...         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]\n",
              "4   ham  ...                                                           [i, have, a, date, on, sunday, with, will]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_Z_Ti2ZEeO",
        "colab_type": "code",
        "outputId": "3ad9914f-6237-442f-f79c-514043e78d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'NLP' == 'nlp'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35REXAgpZEeW",
        "colab_type": "text"
      },
      "source": [
        "### Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVZgz1mGZEeX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "41f70670-afbd-4f77-c15b-9865ed8c585c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopword = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLeq_cXa1Bnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c83fbbd1-d07e-4840-f804-2ce34df5caf4"
      },
      "source": [
        "def remove_stopwords(tokenized_list):\n",
        "  text = [word for word in tokenized_list if word not in stopword]\n",
        "  return text\n",
        "\n",
        "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
        "data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "      <th>body_text_nostop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
              "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
              "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
              "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
              "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
              "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
              "      <td>[date, sunday]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                     body_text_nostop\n",
              "0   ham  ...  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...\n",
              "1  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
              "2   ham  ...                                                 [nah, dont, think, goes, usf, lives, around, though]\n",
              "3   ham  ...                                              [even, brother, like, speak, treat, like, aids, patent]\n",
              "4   ham  ...                                                                                       [date, sunday]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruzLk_Fj4v0d",
        "colab_type": "text"
      },
      "source": [
        "# Supplemental Data Cleaning: Using Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaFCRn_y46Cq",
        "colab_type": "text"
      },
      "source": [
        "### Test out Porter stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw04PXyR2UwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = nltk.PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EBX97XV5BS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "250b4122-bf48-49de-ea4f-3a670d6de2a9"
      },
      "source": [
        "print(ps.stem('grows'))\n",
        "print(ps.stem('growing'))\n",
        "print(ps.stem('grow'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grow\n",
            "grow\n",
            "grow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l3Bh6mT5FNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "668d6888-58f4-446f-cf58-2caf5cdf79bd"
      },
      "source": [
        "print(ps.stem('run'))\n",
        "print(ps.stem('running'))\n",
        "print(ps.stem('runner'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "runner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGyWj3s45lfC",
        "colab_type": "text"
      },
      "source": [
        "## Read in raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpkl_J215eU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a9e926d8-4131-48c5-ece2-57237eec2e2c"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
        "data.columns = ['label', 'body_text']\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                            body_text\n",
              "0   ham  I've been searching for the right words to thank you for this breather. I promise i wont take yo...\n",
              "1  spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
              "2   ham                                        Nah I don't think he goes to usf, he lives around here though\n",
              "3   ham                        Even my brother is not like to speak with me. They treat me like aids patent.\n",
              "4   ham                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnmeNU1S6Ss5",
        "colab_type": "text"
      },
      "source": [
        "## Clean up text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA9dNmw07iEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJGdBsw16EUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c834b04b-8928-4220-a551-6928ad2fdde8"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = \"\".join([word for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+', text)\n",
        "  text = [word for word in tokens if word not in stopwords]\n",
        "  return text\n",
        "\n",
        "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_nostop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>[date, sunday]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                     body_text_nostop\n",
              "0   ham  ...  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...\n",
              "1  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
              "2   ham  ...                                                 [nah, dont, think, goes, usf, lives, around, though]\n",
              "3   ham  ...                                              [even, brother, like, speak, treat, like, aids, patent]\n",
              "4   ham  ...                                                                                       [date, sunday]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mLdl4he7lPM",
        "colab_type": "text"
      },
      "source": [
        "## Stem text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40L32NWi7FNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9ce3a4cb-1e04-4d58-c425-67ee52e18090"
      },
      "source": [
        "def stemming(tokenized_text):\n",
        "  text = [ps.stem(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
        "data.head()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_nostop</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
              "      <td>[ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>[date, sunday]</td>\n",
              "      <td>[date, sunday]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                    body_text_stemmed\n",
              "0   ham  ...  [ive, search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, won...\n",
              "1  spam  ...  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...\n",
              "2   ham  ...                                                   [nah, dont, think, goe, usf, live, around, though]\n",
              "3   ham  ...                                               [even, brother, like, speak, treat, like, aid, patent]\n",
              "4   ham  ...                                                                                       [date, sunday]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0x1OqXC_uT",
        "colab_type": "text"
      },
      "source": [
        "## Supplemental Data Cleaning: Using a Lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK2Q73RGDG51",
        "colab_type": "text"
      },
      "source": [
        "### Test out WordNet lemmatizer (read more about WordNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpIH2CPjDFbH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "14732807-3cf5-453a-b641-5e4d84e1c84f"
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbAt43VX7-Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD7Z8ChbEA5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7f4eab61-b7bf-4de2-cff7-04d19a96e41c"
      },
      "source": [
        "print(ps.stem('meanness'))\n",
        "print(ps.stem('meaning'))\n",
        "\n",
        "print(ps.stem('goose'))\n",
        "print(ps.stem('geese'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean\n",
            "mean\n",
            "goos\n",
            "gees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41vzMXuYEKY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3d1a1387-4cb4-488c-a93b-42b312dc390c"
      },
      "source": [
        "print(wn.lemmatize('meanness'))\n",
        "print(wn.lemmatize('meaning'))\n",
        "\n",
        "print(wn.lemmatize('goose'))\n",
        "print(wn.lemmatize('geese'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meanness\n",
            "meaning\n",
            "goose\n",
            "goose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTb9C3YOE-ZK",
        "colab_type": "text"
      },
      "source": [
        "### Read Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmr7eEQHEh1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0680b1f5-92f3-4b63-d428-c28aa0cfa702"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
        "data.columns = ['label', 'body_text']\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                                                            body_text\n",
              "0   ham  I've been searching for the right words to thank you for this breather. I promise i wont take yo...\n",
              "1  spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...\n",
              "2   ham                                        Nah I don't think he goes to usf, he lives around here though\n",
              "3   ham                        Even my brother is not like to speak with me. They treat me like aids patent.\n",
              "4   ham                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaJvz6r5FJqK",
        "colab_type": "text"
      },
      "source": [
        "### Clean up Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTKPBVCfFBvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK2J1O4-FMH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3096ba89-eca8-4fa7-f0e4-ebbc0b39482d"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = \"\".join([word for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+', text)\n",
        "  text = [word for word in tokens if word not in stopwords]\n",
        "  return text\n",
        "\n",
        "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_nostop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>[date, sunday]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                     body_text_nostop\n",
              "0   ham  ...  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...\n",
              "1  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
              "2   ham  ...                                                 [nah, dont, think, goes, usf, lives, around, though]\n",
              "3   ham  ...                                              [even, brother, like, speak, treat, like, aids, patent]\n",
              "4   ham  ...                                                                                       [date, sunday]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocR8MbElFWt-",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatize Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hQatappFSpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "a3b624a0-ed82-4f83-df21-befec83b7b4c"
      },
      "source": [
        "def lemmatizing(tokenized_text):\n",
        "  text = [wn.lemmatize(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
        "data.head(10)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>body_text</th>\n",
              "      <th>body_text_nostop</th>\n",
              "      <th>body_text_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
              "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
              "      <td>[ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
              "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
              "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
              "      <td>[date, sunday]</td>\n",
              "      <td>[date, sunday]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
              "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
              "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have been selected to receivea 900 prize reward! To c...</td>\n",
              "      <td>[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...</td>\n",
              "      <td>[winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with came...</td>\n",
              "      <td>[mobile, 11, months, u, r, entitled, update, latest, colour, mobiles, camera, free, call, mobile...</td>\n",
              "      <td>[mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ham</td>\n",
              "      <td>I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried ...</td>\n",
              "      <td>[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>\n",
              "      <td>[im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>SIX chances to win CASH! From 100 to 20,000 pounds txt&gt; CSH11 and send to 87575. Cost 150p/day, ...</td>\n",
              "      <td>[six, chances, win, cash, 100, 20000, pounds, txt, csh11, send, 87575, cost, 150pday, 6days, 16,...</td>\n",
              "      <td>[six, chance, win, cash, 100, 20000, pound, txt, csh11, send, 87575, cost, 150pday, 6days, 16, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                                                                 body_text_lemmatized\n",
              "0   ham  ...  [ive, searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promi...\n",
              "1  spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
              "2   ham  ...                                                    [nah, dont, think, go, usf, life, around, though]\n",
              "3   ham  ...                                               [even, brother, like, speak, treat, like, aid, patent]\n",
              "4   ham  ...                                                                                       [date, sunday]\n",
              "5   ham  ...  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...\n",
              "6  spam  ...  [winner, valued, network, customer, selected, receivea, 900, prize, reward, claim, call, 0906170...\n",
              "7  spam  ...  [mobile, 11, month, u, r, entitled, update, latest, colour, mobile, camera, free, call, mobile, ...\n",
              "8   ham  ...     [im, gonna, home, soon, dont, want, talk, stuff, anymore, tonight, k, ive, cried, enough, today]\n",
              "9  spam  ...  [six, chance, win, cash, 100, 20000, pound, txt, csh11, send, 87575, cost, 150pday, 6days, 16, t...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhnKRKgdF3YQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
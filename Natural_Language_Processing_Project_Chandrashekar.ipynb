{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural_Language_Processing_Project_Chandrashekar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMo32NkfdNxm0tisCX1BKic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChandrashekarCYoga/NLP/blob/master/Natural_Language_Processing_Project_Chandrashekar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKlpT10Znknw",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Movie Reviews Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ_wHkJ7oQa1",
        "colab_type": "text"
      },
      "source": [
        "Firstly, let's select TensorFlow version 2.x in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8vuoWiOn09C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8206a064-e6d7-4c79-bc1e-c43b86c20d72"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7D5nToNoYCP",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset from keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszQNAUCoTQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2599d682-3130-4ac5-c74b-f1111970eeeb"
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3GgWe5JowM9",
        "colab_type": "text"
      },
      "source": [
        "### Prepare input and output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc-zAlYWomw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 # max vocab size as per the problem statement\n",
        "maxlen = 300 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ib-8VYPslws",
        "colab_type": "text"
      },
      "source": [
        "## **Train and Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wer7EGHFpWad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset as a list of ints\n",
        "\n",
        "# vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukFQj3Xptv64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "59a074bb-17ee-4156-8d51-2b109af28ca6"
      },
      "source": [
        "x_train[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    1,  194, 1153,  194, 8255,   78,  228,    5,    6, 1463,\n",
              "       4369, 5012,  134,   26,    4,  715,    8,  118, 1634,   14,  394,\n",
              "         20,   13,  119,  954,  189,  102,    5,  207,  110, 3103,   21,\n",
              "         14,   69,  188,    8,   30,   23,    7,    4,  249,  126,   93,\n",
              "          4,  114,    9, 2300, 1523,    5,  647,    4,  116,    9,   35,\n",
              "       8163,    4,  229,    9,  340, 1322,    4,  118,    9,    4,  130,\n",
              "       4901,   19,    4, 1002,    5,   89,   29,  952,   46,   37,    4,\n",
              "        455,    9,   45,   43,   38, 1543, 1905,  398,    4, 1649,   26,\n",
              "       6853,    5,  163,   11, 3215,    2,    4, 1153,    9,  194,  775,\n",
              "          7, 8255,    2,  349, 2637,  148,  605,    2, 8003,   15,  123,\n",
              "        125,   68,    2, 6853,   15,  349,  165, 4362,   98,    5,    4,\n",
              "        228,    9,   43,    2, 1157,   15,  299,  120,    5,  120,  174,\n",
              "         11,  220,  175,  136,   50,    9, 4373,  228, 8255,    5,    2,\n",
              "        656,  245, 2350,    5,    4, 9837,  131,  152,  491,   18,    2,\n",
              "         32, 7464, 1212,   14,    9,    6,  371,   78,   22,  625,   64,\n",
              "       1382,    9,    8,  168,  145,   23,    4, 1690,   15,   16,    4,\n",
              "       1355,    5,   28,    6,   52,  154,  462,   33,   89,   78,  285,\n",
              "         16,  145,   95], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd-7luTat-cA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23bc2df6-4898-463e-d5f9-6d435881a360"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3JSY1mCuE0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e17fe5d-c614-4f8e-b9e6-e5e06c5c55a7"
      },
      "source": [
        "x_train[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktp6v-sruIZ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "418f9a3a-7977-40dc-dfcb-6278e340edf7"
      },
      "source": [
        "y_train[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12TN4uiL5XZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3eb035d6-8ddd-4427-a40d-c0e711e31639"
      },
      "source": [
        "import numpy as np\n",
        "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     1]\n",
            " [12500 12500]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMULGzd6k6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6111020-7860-47c5-b62c-8ded3bff36ba"
      },
      "source": [
        "import numpy as np\n",
        "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     1]\n",
            " [12500 12500]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tspVgLoV7Kke",
        "colab_type": "text"
      },
      "source": [
        "## **Word Index Building**\n",
        "### Get the word index and then Create a key-value pair for word and word_id \n",
        "Convert predict sequence back to words in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqlsByJ36vmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqrDdZjp7kGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_map = dict(map(reversed, word_index.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjgh0nG27zI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0973a198-fca9-4d1c-9b5c-c1244fad2e44"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "\n",
        "n_items = take(20, word_map.items())\n",
        "n_items"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(34701, 'fawn'),\n",
              " (52006, 'tsukino'),\n",
              " (52007, 'nunnery'),\n",
              " (16816, 'sonja'),\n",
              " (63951, 'vani'),\n",
              " (1408, 'woods'),\n",
              " (16115, 'spiders'),\n",
              " (2345, 'hanging'),\n",
              " (2289, 'woody'),\n",
              " (52008, 'trawling'),\n",
              " (52009, \"hold's\"),\n",
              " (11307, 'comically'),\n",
              " (40830, 'localized'),\n",
              " (30568, 'disobeying'),\n",
              " (52010, \"'royale\"),\n",
              " (40831, \"harpo's\"),\n",
              " (52011, 'canet'),\n",
              " (19313, 'aileen'),\n",
              " (52012, 'acurately'),\n",
              " (52013, \"diplomat's\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjPA82d775sa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a1c102e-7b5d-4586-86cc-bf2dc27bd503"
      },
      "source": [
        "# Function takes a tokenized sentence and returns the words\n",
        "def sequence_to_text(list_of_indices):\n",
        "    # Looking up words in dictionary\n",
        "    words = [word_map.get(word) for word in list_of_indices]\n",
        "    return(words)\n",
        "\n",
        "print(sequence_to_text(x_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'musicians', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'landed', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'frog', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'barrel', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee5Ef8xi-ahx",
        "colab_type": "text"
      },
      "source": [
        "## **Build a Sequential Model using Keras for the Sentiment Classification**\n",
        "\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef3Dcjyu9r3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSUZRpwsGncE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "embedding_dims = 128\n",
        "nb_filter = 250\n",
        "filter_length = 3\n",
        "hidden_dims = 250\n",
        "nb_epoch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPDAw6M6_YXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "91534ff0-cf6c-4fa7-cddb-cd021588748b"
      },
      "source": [
        "# Creat the model \n",
        "\n",
        "### create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dims, trainable=True, input_length=maxlen))\n",
        "model.add(LSTM(units=64, dropout=0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,331,521\n",
            "Trainable params: 1,331,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw-v4VDwHcoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "ef07800f-9770-4e3c-ac26-68f324b2e4c1"
      },
      "source": [
        "### Fit the model\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=nb_epoch, batch_size=batch_size, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 231s 9ms/step - loss: 0.3961 - accuracy: 0.8235 - val_loss: 0.3927 - val_accuracy: 0.8372\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 237s 9ms/step - loss: 0.2458 - accuracy: 0.9024 - val_loss: 0.2996 - val_accuracy: 0.8786\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 238s 10ms/step - loss: 0.2271 - accuracy: 0.9089 - val_loss: 0.3628 - val_accuracy: 0.8564\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 237s 9ms/step - loss: 0.1454 - accuracy: 0.9457 - val_loss: 0.3648 - val_accuracy: 0.8720\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 232s 9ms/step - loss: 0.1040 - accuracy: 0.9635 - val_loss: 0.4619 - val_accuracy: 0.8673\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 229s 9ms/step - loss: 0.0886 - accuracy: 0.9696 - val_loss: 0.4587 - val_accuracy: 0.8642\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 229s 9ms/step - loss: 0.0577 - accuracy: 0.9808 - val_loss: 0.4742 - val_accuracy: 0.8527\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 229s 9ms/step - loss: 0.0446 - accuracy: 0.9858 - val_loss: 0.6097 - val_accuracy: 0.8640\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 228s 9ms/step - loss: 0.0402 - accuracy: 0.9866 - val_loss: 0.5717 - val_accuracy: 0.8630\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 228s 9ms/step - loss: 0.0508 - accuracy: 0.9829 - val_loss: 0.5443 - val_accuracy: 0.8612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fdf9d9b8828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XpETPVlwQNj",
        "colab_type": "text"
      },
      "source": [
        "## **Model Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZhESKFJg-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffadbbe0-5c63-48dd-87be-57e8a97b47b0"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxHQRYLywUSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model prediction\n",
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woeH2C2Awo6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "042cea61-0611-4e9e-dffb-a426b35d6bf8"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.2238235e-03]\n",
            " [9.9999350e-01]\n",
            " [3.1647015e-01]\n",
            " ...\n",
            " [6.8342686e-04]\n",
            " [6.7372918e-02]\n",
            " [9.9977994e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6sGpvwqxDgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = np.round(y_pred, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNIvrjWexH_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbbbf99a-c968-4e71-db34-4dcff39feb5c"
      },
      "source": [
        "y_pred = y_pred.ravel()\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLJgSzTHxQAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = y_pred.astype('int64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaya1htrxZ1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b8ab168-9221-4710-f457-d4413d2e049f"
      },
      "source": [
        "y_test.ravel\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24-sPoQZxevi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3e17a9f5-75ed-4f44-b5eb-ada259b49b42"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['Sentiment_Positive', 'Sentiment_Negative']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Sentiment_Positive       0.85      0.88      0.86     12500\n",
            "Sentiment_Negative       0.87      0.85      0.86     12500\n",
            "\n",
            "          accuracy                           0.86     25000\n",
            "         macro avg       0.86      0.86      0.86     25000\n",
            "      weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BJgAodKxv9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab0e3258-c3bb-4a29-ca4f-92a925790b78"
      },
      "source": [
        "sequence_to_text(x_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " 'the',\n",
              " 'wonder',\n",
              " 'own',\n",
              " 'as',\n",
              " 'by',\n",
              " 'is',\n",
              " 'sequence',\n",
              " 'i',\n",
              " 'i',\n",
              " 'and',\n",
              " 'and',\n",
              " 'to',\n",
              " 'of',\n",
              " 'hollywood',\n",
              " 'br',\n",
              " 'of',\n",
              " 'down',\n",
              " 'shouting',\n",
              " 'getting',\n",
              " 'boring',\n",
              " 'of',\n",
              " 'ever',\n",
              " 'it',\n",
              " 'sadly',\n",
              " 'sadly',\n",
              " 'sadly',\n",
              " 'i',\n",
              " 'i',\n",
              " 'was',\n",
              " 'then',\n",
              " 'does',\n",
              " \"don't\",\n",
              " 'close',\n",
              " 'faint',\n",
              " 'after',\n",
              " 'one',\n",
              " 'carry',\n",
              " 'as',\n",
              " 'by',\n",
              " 'are',\n",
              " 'be',\n",
              " 'favourites',\n",
              " 'all',\n",
              " 'family',\n",
              " 'turn',\n",
              " 'in',\n",
              " 'does',\n",
              " 'as',\n",
              " 'three',\n",
              " 'part',\n",
              " 'in',\n",
              " 'another',\n",
              " 'some',\n",
              " 'to',\n",
              " 'be',\n",
              " 'probably',\n",
              " 'with',\n",
              " 'world',\n",
              " 'and',\n",
              " 'her',\n",
              " 'an',\n",
              " 'have',\n",
              " 'faint',\n",
              " 'beginning',\n",
              " 'own',\n",
              " 'as',\n",
              " 'is',\n",
              " 'sequence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poBaE0gcyVPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "034e531d-82da-4ec6-ab0d-a44d07649ef2"
      },
      "source": [
        "sequence_to_text(x_test[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " 'the',\n",
              " 'as',\n",
              " 'you',\n",
              " \"world's\",\n",
              " 'is',\n",
              " 'quite',\n",
              " 'br',\n",
              " 'mankind',\n",
              " 'most',\n",
              " 'that',\n",
              " 'quest',\n",
              " 'are',\n",
              " 'chase',\n",
              " 'to',\n",
              " 'being',\n",
              " 'quickly',\n",
              " 'of',\n",
              " 'little',\n",
              " 'it',\n",
              " 'time',\n",
              " 'hell',\n",
              " 'to',\n",
              " 'plot',\n",
              " 'br',\n",
              " 'of',\n",
              " 'something',\n",
              " 'long',\n",
              " 'put',\n",
              " 'are',\n",
              " 'of',\n",
              " 'every',\n",
              " 'place',\n",
              " 'this',\n",
              " 'consequence',\n",
              " 'and',\n",
              " 'of',\n",
              " 'interplay',\n",
              " 'storytelling',\n",
              " 'being',\n",
              " 'nasty',\n",
              " 'not',\n",
              " 'of',\n",
              " 'you',\n",
              " 'warren',\n",
              " 'in',\n",
              " 'is',\n",
              " 'failed',\n",
              " 'club',\n",
              " 'i',\n",
              " 'i',\n",
              " 'of',\n",
              " 'films',\n",
              " 'pay',\n",
              " 'so',\n",
              " 'sequences',\n",
              " 'and',\n",
              " 'film',\n",
              " 'okay',\n",
              " 'uses',\n",
              " 'to',\n",
              " 'received',\n",
              " 'and',\n",
              " 'if',\n",
              " 'time',\n",
              " 'done',\n",
              " 'for',\n",
              " 'room',\n",
              " 'sugar',\n",
              " 'viewer',\n",
              " 'as',\n",
              " 'cartoon',\n",
              " 'of',\n",
              " 'gives',\n",
              " 'to',\n",
              " 'forgettable',\n",
              " 'br',\n",
              " 'be',\n",
              " 'because',\n",
              " 'many',\n",
              " 'these',\n",
              " 'of',\n",
              " 'reflection',\n",
              " 'sugar',\n",
              " 'contained',\n",
              " 'gives',\n",
              " 'it',\n",
              " 'wreck',\n",
              " 'scene',\n",
              " 'to',\n",
              " 'more',\n",
              " 'was',\n",
              " 'two',\n",
              " 'when',\n",
              " 'had',\n",
              " 'find',\n",
              " 'as',\n",
              " 'you',\n",
              " 'another',\n",
              " 'it',\n",
              " 'of',\n",
              " 'themselves',\n",
              " 'probably',\n",
              " 'who',\n",
              " 'interplay',\n",
              " 'storytelling',\n",
              " 'if',\n",
              " 'itself',\n",
              " 'by',\n",
              " 'br',\n",
              " 'about',\n",
              " \"1950's\",\n",
              " 'films',\n",
              " 'not',\n",
              " 'would',\n",
              " 'effects',\n",
              " 'that',\n",
              " 'her',\n",
              " 'box',\n",
              " 'to',\n",
              " 'miike',\n",
              " 'for',\n",
              " 'if',\n",
              " 'hero',\n",
              " 'close',\n",
              " 'seek',\n",
              " 'end',\n",
              " 'is',\n",
              " 'very',\n",
              " 'together',\n",
              " 'movie',\n",
              " 'of',\n",
              " 'wheel',\n",
              " 'got',\n",
              " 'say',\n",
              " 'kong',\n",
              " 'sugar',\n",
              " 'fred',\n",
              " 'close',\n",
              " 'bore',\n",
              " 'there',\n",
              " 'is',\n",
              " 'playing',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'and',\n",
              " 'pan',\n",
              " 'place',\n",
              " 'trilogy',\n",
              " 'of',\n",
              " 'lacks',\n",
              " 'br',\n",
              " 'of',\n",
              " 'their',\n",
              " 'time',\n",
              " 'much',\n",
              " 'this',\n",
              " 'men',\n",
              " 'as',\n",
              " 'on',\n",
              " 'it',\n",
              " 'is',\n",
              " 'telling',\n",
              " 'program',\n",
              " 'br',\n",
              " 'silliness',\n",
              " 'okay',\n",
              " 'and',\n",
              " 'to',\n",
              " 'frustration',\n",
              " 'at',\n",
              " 'corner',\n",
              " 'and',\n",
              " 'she',\n",
              " 'of',\n",
              " 'sequences',\n",
              " 'to',\n",
              " 'political',\n",
              " 'clearly',\n",
              " 'in',\n",
              " 'of',\n",
              " 'drugs',\n",
              " 'keep',\n",
              " 'guy',\n",
              " 'i',\n",
              " 'i',\n",
              " 'was',\n",
              " 'throwing',\n",
              " 'room',\n",
              " 'sugar',\n",
              " 'as',\n",
              " 'it',\n",
              " 'by',\n",
              " 'br',\n",
              " 'be',\n",
              " 'plot',\n",
              " 'many',\n",
              " 'for',\n",
              " 'occasionally',\n",
              " 'film',\n",
              " 'verge',\n",
              " 'boyfriend',\n",
              " 'difficult',\n",
              " 'kid',\n",
              " 'as',\n",
              " 'you',\n",
              " 'it',\n",
              " 'failed',\n",
              " 'not',\n",
              " 'if',\n",
              " 'gerard',\n",
              " 'to',\n",
              " 'if',\n",
              " 'woman',\n",
              " 'in',\n",
              " 'and',\n",
              " 'is',\n",
              " 'police',\n",
              " 'fi',\n",
              " 'spooky',\n",
              " 'or',\n",
              " 'of',\n",
              " 'self',\n",
              " 'what',\n",
              " 'have',\n",
              " 'pretty',\n",
              " 'in',\n",
              " 'can',\n",
              " 'so',\n",
              " 'suit',\n",
              " 'you',\n",
              " 'good',\n",
              " '2',\n",
              " 'which',\n",
              " 'why',\n",
              " 'super',\n",
              " 'as',\n",
              " 'it',\n",
              " 'main',\n",
              " 'of',\n",
              " 'my',\n",
              " 'i',\n",
              " 'i',\n",
              " '\\x96',\n",
              " 'if',\n",
              " 'time',\n",
              " 'screenplay',\n",
              " 'in',\n",
              " 'same',\n",
              " 'this',\n",
              " 'remember',\n",
              " 'assured',\n",
              " 'have',\n",
              " 'action',\n",
              " 'one',\n",
              " 'in',\n",
              " 'realistic',\n",
              " 'that',\n",
              " 'better',\n",
              " 'of',\n",
              " 'lessons']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7taw-byl5f",
        "colab_type": "text"
      },
      "source": [
        "## **Retrive the output of each layer in keras for a given single test sample from the trained model you built**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGpcbEddyqs-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "be052510-b546-40fa-937b-efe8a4f5f233"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# input placeholder\n",
        "ip = model.input\n",
        "\n",
        "# all output layers\n",
        "outputs = [layer.output for layer in model.layers]\n",
        "\n",
        "print(outputs)\n",
        "functors = [K.function([ip, K.learning_phase()], [out]) for out in outputs]    # evaluation functions\n",
        "\n",
        "\n",
        "# Testing\n",
        "test = x_test[0][np.newaxis, ...]\n",
        "layer_outs = [func([test, 1]) for func in functors]\n",
        "\n",
        "print (layer_outs)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'embedding_1/embedding_lookup/Identity_1:0' shape=(None, 300, 128) dtype=float32>, <tf.Tensor 'lstm_1/strided_slice_18:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'dense_1/Relu:0' shape=(None, 32) dtype=float32>, <tf.Tensor 'dense_2/Sigmoid:0' shape=(None, 1) dtype=float32>]\n",
            "[[array([[[-0.08002027,  0.10763775, -0.08623564, ...,  0.04880287,\n",
            "          0.00209026,  0.02427271],\n",
            "        [-0.08002027,  0.10763775, -0.08623564, ...,  0.04880287,\n",
            "          0.00209026,  0.02427271],\n",
            "        [-0.08002027,  0.10763775, -0.08623564, ...,  0.04880287,\n",
            "          0.00209026,  0.02427271],\n",
            "        ...,\n",
            "        [ 0.01053442, -0.0711675 ,  0.11472849, ...,  0.03883199,\n",
            "         -0.09521551,  0.01024593],\n",
            "        [-0.04420754, -0.00559954,  0.08075345, ...,  0.01223716,\n",
            "          0.08970833, -0.01524084],\n",
            "        [-0.02831219,  0.05909333, -0.16075805, ...,  0.10717237,\n",
            "         -0.0083857 , -0.13487203]]], dtype=float32)], [array([[ 2.8007114e-01,  4.9990335e-01,  1.6693561e-01, -4.1980105e-03,\n",
            "        -4.1304761e-01, -1.1014820e-02,  1.5829337e-01, -4.1131005e-02,\n",
            "        -3.1332403e-01,  3.7152637e-02, -4.9167767e-01,  8.3578795e-02,\n",
            "        -8.4532470e-02,  1.8666782e-04,  1.1487918e-01, -1.4317562e-02,\n",
            "         1.8100249e-02,  2.2771740e-02, -6.8606395e-01, -3.7522522e-01,\n",
            "        -7.8735590e-02,  1.3610892e-02,  4.1597176e-01, -4.9039196e-02,\n",
            "         1.1803748e-01, -1.4347314e-02, -7.1368642e-02,  7.7350535e-03,\n",
            "        -4.3818496e-02, -8.3213881e-02, -3.1478226e-01, -1.6230447e-02,\n",
            "         3.0608329e-01, -1.3146540e-02,  6.7515150e-03,  6.3102990e-02,\n",
            "         3.9926293e-01,  4.4145502e-02,  3.2300991e-01,  5.4165204e-03,\n",
            "        -1.2996737e-02,  2.5911227e-02,  5.9594156e-04,  1.2830841e-02,\n",
            "         2.4357262e-01, -3.0075532e-01, -3.7934057e-02,  1.1588850e-01,\n",
            "         4.6123669e-02,  8.1834532e-03,  5.9269644e-02, -3.8936503e-02,\n",
            "        -1.0111999e-02, -3.2358352e-02,  5.3223759e-01,  1.2637989e-02,\n",
            "         3.8883688e-03, -1.4230239e-01,  4.4530326e-01, -2.3732570e-03,\n",
            "        -1.0197181e-02, -3.0438092e-01,  4.2491806e-01,  5.8238097e-02]],\n",
            "      dtype=float32)], [array([[1.2726202 , 0.        , 0.        , 0.        , 0.        ,\n",
            "        0.67809623, 1.3268635 , 0.        , 0.6490365 , 0.        ,\n",
            "        0.9869801 , 0.        , 0.        , 0.        , 1.530607  ,\n",
            "        0.8961138 , 0.82655835, 1.2011554 , 0.09334371, 0.        ,\n",
            "        0.        , 0.6360943 , 0.1300252 , 0.        , 0.76004094,\n",
            "        0.9002237 , 0.14663014, 0.        , 0.40187377, 0.67393637,\n",
            "        1.1945987 , 1.4510591 ]], dtype=float32)], [array([[0.00382206]], dtype=float32)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQD5_FfT3Awf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}